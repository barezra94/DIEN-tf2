# -*- coding: utf-8 -*-
"""ml_prepro.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PL22zExzbe9IbF1rtRo7RhgX2s2_DNY4
"""

import pandas as pd
import random
import numpy as np
import pickle
import csv
import tensorflow as tf

dataset_dir = '/data/private/Ad/amazon/np_prepro/'
with open(dataset_dir+'dataset.pkl', 'rb') as f:
    train_set = pickle.load(f, encoding='latin1') # uid, [hist], vid, label
    test_set = pickle.load(f, encoding='latin1')  # uid, [hist], pid, nid
    cate_list = pickle.load(f, encoding='latin1') # id2cate list
    cate_list = tf.convert_to_tensor(cate_list, dtype=tf.int64)
    user_count, item_count, cate_count = pickle.load(f) # (user_count, item_count, cate_count)

root = '/data/private/Ad/ml-20m/'
reviews_df  = pd.read_csv(root+'ratings.csv')

with open(root+'np_prepro/reviews.pkl', 'wb') as f:
    pickle.dump(reviews_df, f, pickle.HIGHEST_PROTOCOL)

meta_df = pd.read_csv(root+'movies.csv')

meta_df[meta_df['movieId'].isin(reviews_df['movieId'].unique())]
meta_df = meta_df.reset_index(drop=True)
with open(root+'np_prepro/meta.pkl', 'wb') as f:
    pickle.dump(meta_df, f, pickle.HIGHEST_PROTOCOL)

meta_df

# with open(root+'np_prepro/reviews.pkl', 'rb') as f:
#     reviews_df = pickle.load(f)
# with open(root+'np_prepro/meta.pkl', 'rb') as f:
#     meta_df = pickle.load(f)

reviews_df = reviews_df[['userId','movieId','rating','timestamp']]
reviews_df.loc[:,'rating'] = reviews_df['rating'].map(lambda x: 1 if x > 3 else 0)
meta_df = meta_df[['movieId', 'genres']]
meta_df.loc[:,'genres'] = meta_df['genres'].map(lambda x: x.split('|')[0])

def build_map(df, col_name):
    key = sorted(df[col_name].unique().tolist())
    m = dict(zip(key, range(len(key))))
    df.loc[:,col_name] = df[col_name].map(lambda x: m[x])
    return m, key

vid_map, vid_key = build_map(meta_df, 'movieId')
cat_map, cat_key = build_map(meta_df, 'genres')
uid_map, uid_key = build_map(reviews_df, 'userId')

user_count, item_count, cate_count, example_count =\
    len(uid_map), len(vid_map), len(cat_map), reviews_df.shape[0]
print('user_count: %d\titem_count: %d\tcate_count: %d\texample_count: %d' %
      (user_count, item_count, cate_count, example_count))

meta_df = meta_df.sort_values('movieId')
meta_df = meta_df.reset_index(drop=True)

reviews_df['movieId'] = reviews_df['movieId'].map(lambda x: vid_map[x])
reviews_df = reviews_df.sort_values(['userId', 'timestamp'])
reviews_df = reviews_df.reset_index(drop=True)

cate_list = [meta_df['genres'][i] for i in range(len(vid_map))]
cate_list = np.array(cate_list, dtype=np.int32)

with open(root+'np_prepro/remap.pkl', 'wb') as f:
    pickle.dump(reviews_df, f, pickle.HIGHEST_PROTOCOL) # uid, iid
    pickle.dump(cate_list, f, pickle.HIGHEST_PROTOCOL) # cid of iid line
    pickle.dump((user_count, item_count, cate_count, example_count),
              f, pickle.HIGHEST_PROTOCOL)
    pickle.dump((vid_key, cat_key, uid_key), f, pickle.HIGHEST_PROTOCOL)

pos_cnt, neg_cnt = 0, 0
for userId, hist in reviews_df.groupby('userId'):
    movie_list = hist['movieId'].tolist()
    label_list = hist['rating'].tolist()

    pos_cnt += sum(label_list)
    neg_cnt += len(label_list) - sum(label_list)
    
print(pos_cnt, neg_cnt, pos_cnt/(pos_cnt+neg_cnt))

random.seed(1234)

train_set = []
test_set = []
train_count = 100000
train_user = np.random.choice(user_count, train_count, replace=False)
for userId, hist in reviews_df.groupby('userId'):
    timestamp_list = hist['timestamp'].tolist()
    movie_list = hist['movieId'].tolist()
    label_list = hist['rating'].tolist()
    pos_list, neg_list = [], []
    for i, (v,r) in enumerate(zip(movie_list, label_list)):
        if r == 1: pos_list.append(v);
        else: neg_list.append(v)
    
    if len(pos_list) > len(neg_list):
        for _ in range(len(pos_list)-len(neg_list)):
            neg = pos_list[0]
            while neg in pos_list + neg_list :
                neg = random.randint(0, item_count-1)
            neg_list.append(neg)

    if userId in train_user:
        for i in range(1, len(pos_list)):
            hist = pos_list[:i]
            train_set.append((userId, hist, pos_list[i], 1))
            train_set.append((userId, hist, neg_list[i], 0))
    else:
        for i in range(1, len(pos_list)):
            hist = movie_list[:i]
            label = (pos_list[i], neg_list[i])
            test_set.append((userId, hist, label))

random.shuffle(train_set)
random.shuffle(test_set)

with open(root+'np_prepro/dataset.pkl', 'wb') as f:
    pickle.dump(train_set, f, pickle.HIGHEST_PROTOCOL)
    pickle.dump(test_set, f, pickle.HIGHEST_PROTOCOL)
    pickle.dump(cate_list, f, pickle.HIGHEST_PROTOCOL)
    pickle.dump((user_count, item_count, cate_count), f, pickle.HIGHEST_PROTOCOL)

with open(root+'dataset.pkl', 'wb') as f:
    pickle.dump(train_set, f, protocol=2)
    pickle.dump(test_set, f, protocol=2)
    pickle.dump(cate_list, f, protocol=2)
    pickle.dump((user_count, item_count, cate_count), f, protocol=2)